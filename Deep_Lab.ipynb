{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90779671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from utils import *\n",
    "import albumentations as album\n",
    "#import extractors\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm.auto import tqdm as tq\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef2f49",
   "metadata": {},
   "source": [
    "# Preprocessing required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32432f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth\" to C:\\Users\\rkroc/.cache\\torch\\hub\\checkpoints\\semi_supervised_resnext101_32x4-dc43570a.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5426b7b211ca4f02a49beeb1410914f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to be run only one time\n",
    "ENCODER = 'resnext101_32x4d'\n",
    "ENCODER_WEIGHTS = 'ssl'\n",
    "CLASSES = 25\n",
    "ACTIVATION = 'softmax' # could be None for logits or 'softmax2d','softmax' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=25, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image to (256 x 256)\n",
    "# CenterCrop it to (224 x 224)\n",
    "# Convert it to Tensor â€“ all the values in the image will be scaled so they lie between [0, 1]instead of the original, [0, 255] range.\n",
    "# Normalize it with the Imagenet specific values where mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):#plot images in a row\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label 23kjerbfds\n",
    "def one_hot_encode(image,n_classes):\n",
    "    x = F.one_hot(image,n_classes)\n",
    "    return x\n",
    " \n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2,0,1).astype('float32') # convert into tensor\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    _transform = [\n",
    "        album.Lambda(image=preprocessing_fn),\n",
    "        album.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "class BackgroundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,path_m,path_i \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.path_m = mask_path\n",
    "        self.path_i = img_path\n",
    "        self.name = os.listdir(os.path.join(path, 'train_masks'))\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # read images and masks\n",
    "        mask_name = self.name[idx]\n",
    "        mask_path = os.path.join(self.path_m,'train_masks',mask_name)\n",
    "        img_path = os.path.join(self.path_i,'train_images',mask_name.replace('png','jpg'))\n",
    "\n",
    "        \n",
    "#         image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "#         mask = cv2.cvtColor(cv2.imread(mask_path),0)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize((256,256),Image.ANTIALIAS)\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.resize((256,256),Image.ANTIALIAS)\n",
    "#         image = keep_image_size_open(img_path)\n",
    "#         mask = keep_mask_size_open(mask_path)\n",
    "        \n",
    "        image = np.asarray(image).astype('int32')\n",
    "        mask = np.asarray(mask).astype('int32')\n",
    "        mask = np.where(mask<=24,mask,0) # removing every above classes\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "        #one-hot-encode the mask  \n",
    "        mask = torch.from_numpy(mask).to(torch.int64)\n",
    "        mask = one_hot_encode(mask,25)\n",
    "        mask = np.asarray(mask).astype('int32')\n",
    "        \n",
    "\n",
    "        \n",
    "         # preprocessing applied only on numpy array image\n",
    "        sample = self.preprocessing(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "            \n",
    "        return image,mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.name)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = BackgroundDataset('',preprocessing=get_preprocessing(preprocessing_fn))\n",
    "    check_image = data[219][0] # checking for the random 100th image\n",
    "    check_mask = data[219][1]\n",
    "    print(check_image.shape,check_mask.shape)\n",
    "    print(check_image.dtype,check_mask.dtype)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a284f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "nw = 0\n",
    "# Splitting into Train and Val\n",
    "full_dataset = BackgroundDataset('',preprocessing=get_preprocessing(preprocessing_fn))\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size   = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Creating  data_loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs,num_workers=nw,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs,num_workers=nw,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c316377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0005),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "# if os.path.exists('../input/pyramid-scene-parsing-pspnet-resnext50-pytorch/best_model.pth'):\n",
    "#     model = torch.load('../input/pyramid-scene-parsing-pspnet-resnext50-pytorch/best_model.pth', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(val_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model.state_dict(), 'DEEPMIND_model.pt')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ce717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
